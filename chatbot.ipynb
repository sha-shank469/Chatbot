{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMq6V3V7fOFDX/S7R+yBVXk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sha-shank469/Chatbot/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j_5eiouio-Z"
      },
      "source": [
        "pip install tflearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS5MYWRUdyjb"
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5OsKtO1jBT9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import random\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReZi_UzJnpcm"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUTZGsdQoG4G"
      },
      "source": [
        "with open('intents.json') as json_data:\n",
        "  intents=json.load(json_data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3TUIHRYpPWD"
      },
      "source": [
        "intents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtfZt1RbptRB"
      },
      "source": [
        "words=[]\n",
        "classes=[]\n",
        "documents=[]\n",
        "ignore =['?']\n",
        "#loop through each  sentence in the  intents pattern\n",
        "\n",
        "for intent  in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    #tokenize each and every word in sentence\n",
        "    w=nltk.word_tokenize(pattern)\n",
        "    #add word  to words list\n",
        "    words.extend(w)\n",
        "    # add word(s) to documents\n",
        "    documents.append((w,intent['tag']))\n",
        "    #add tags to our classes list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIsTmShgtr2b"
      },
      "source": [
        "#perform  stemming and lower  each word as well as remove duplicates and punchuations contained in the ignore list\n",
        "words =[stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "#remove duplicate classes\n",
        "classes =sorted(list(set(classes)))\n",
        "\n",
        "print(len(documents),\"documents\")\n",
        "print(len(classes),\"classes\",classes)\n",
        "print(len(words),\"unique stemmed words\",words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26f4jvM-xMIL"
      },
      "source": [
        "#create training data\n",
        "training=[]\n",
        "output =[]\n",
        "#create an empty array for output\n",
        "output_empty=[0]*len(classes)\n",
        "#create training set,bag of words for each sentence\n",
        "for doc in documents:\n",
        "  #initialize bag of words\n",
        "  bag=[]\n",
        "  #list of tokenize word for pattern\n",
        "  pattern_words= doc[0]\n",
        "  #stemming each word\n",
        "  pattern_words=[stemmer.stem(word.lower())for word in pattern_words]\n",
        "  #create bag of words array\n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "  #output is 1 for current tag  and '0' for rest of other tags\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(doc[1])] = 1\n",
        "  training.append([bag,output_row])\n",
        "#shuffling features and turning into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "#creating training lists\n",
        "train_x=list(training[:,0])#features data\n",
        "train_y=list(training[:,1])#label data in form of associated tags and intent class\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHzIl42m6_2e"
      },
      "source": [
        "#resetting the underlying graph data\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "#tf.reset_default_graph()\n",
        "#building a Neural netwok:\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])#input layer \n",
        "net = tflearn.fully_connected(net, 10)##hidden layer having 10 nodes\n",
        "net = tflearn.fully_connected(net, 10)##hidden layer having 10 nodes \n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')#output layer ,,here the middle parameter denotes the number of classes.\n",
        "#\n",
        "net = tflearn.regression(net)#to apply regession linear or logistic in order to provided input\n",
        "# Defining model and setting up tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
        "# Start training\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)#higher the number of epoches higher the accurate will be the model\n",
        "model.save('model.tflearn')#saving model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwN36DfKEZZ2"
      },
      "source": [
        "import pickle\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHJNK8x1FLsM"
      },
      "source": [
        "# restoring all the data structures\n",
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1QV4Zr6WH0A"
      },
      "source": [
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpQTjy5cXCoX"
      },
      "source": [
        "# load the saved model\n",
        "model.load('./model.tflearn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNAjKb4lXRLe"
      },
      "source": [
        "# this take input from the user and tokenize it \n",
        "def clean_up_sentence(sentence):\n",
        "    # tokenizing the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stemming each word from cleaning perspective\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGV-qq08X4I3"
      },
      "source": [
        "# returning bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "    # tokenizing the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # generating bag of words\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "\n",
        "    return(np.array(bag))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1jgGWhgYJo0"
      },
      "source": [
        "\n",
        "#this is our response processor\n",
        "ERROR_THRESHOLD = 0.30 #in order to filter out the prediction below this threshold value\n",
        "# we don't want prediction whose value is below .30\n",
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    return return_list"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8T4qaq4c_AJ"
      },
      "source": [
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    # if we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # set context for this intent if necessary\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "                    # check if this intent is contextual and applies to this user's conversation\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: print ('tag:', i['tag'])\n",
        "                        # a random response from the intent\n",
        "                        return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4g7qJCreIrM"
      },
      "source": [
        "classify('What are you hours of operation?')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5w1OWL4eg1x"
      },
      "source": [
        "response('What are you hours of operation?')\n",
        "\n",
        "response('What is menu for today?')#here our model correctly identified the pattern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iVLOBzbvHhY"
      },
      "source": [
        "response('Do you accept Credit Card?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBK2yyXuvLOo"
      },
      "source": [
        "response('Where can we locate you?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfKxIH8cvUfn"
      },
      "source": [
        "response('That is helpful')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyOIjO0-vWX7"
      },
      "source": [
        "response('Bye')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjo2tdzevPyM"
      },
      "source": [
        "context = {}"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUWh0VgfvcPu"
      },
      "source": [
        "\n",
        "ERROR_THRESHOLD = 0.25\n",
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    return return_list"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-MaqGBavhrx"
      },
      "source": [
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    # if we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # set context for this intent if necessary\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "                    # check if this intent is contextual and applies to this user's conversation\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: print ('tag:', i['tag'])\n",
        "                        # a random response from the intent\n",
        "                        return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC-z_vCDvlmK"
      },
      "source": [
        "response('Can you please let me know the delivery options?')\n",
        "\n",
        "response('What is menu for today?')\n",
        "\n",
        "context\n",
        "\n",
        "response(\"Hi there!\", show_details=True)\n",
        "\n",
        "response('What is menu for today?')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}